{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Beat Model Training on Colab (CPSC2021)\n",
                "\n",
                "This notebook trains a CNN model for ECG beat classification using the **CPSC2021** dataset.\n",
                "\n",
                "**Key Features:**\n",
                "- **Dataset**: CPSC2021 (The 4th China Physiological Signal Challenge 2021)\n",
                "- **Resolution**: Native 200Hz\n",
                "- **Classification**: Hierarchical (Sub-primitives -> Top-level)\n",
                "\n",
                "**Annotations:**\n",
                "We use the standard PhysioBank annotation codes (e.g., 'N', 'V', 'S'). \n",
                "See the [PhysioNet Annotations Reference](https://archive.physionet.org/physiobank/annotations.shtml) for a complete list.\n",
                "\n",
                "**Note:** CPSC2021 is primarily for AFib detection, but we attempt to extract beat-level annotations if available."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install tensorflow numpy pandas scipy wfdb scikit-learn pyedflib\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data Setup (Google Drive ONLY)\n",
                "import os\n",
                "import subprocess\n",
                "import sys\n",
                "from google.colab import drive\n",
                "\n",
                "# 1. Mount Google Drive\n",
                "# This will prompt for authorization in the Colab output\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Configuration\n",
                "# Upload your CPSC2021 zip to this path in Drive BEFORE running this cell\n",
                "DRIVE_ZIP_PATH = \"/content/drive/ML/CPSC2021/Training_set_I.zip\"\n",
                "EXTRACT_DIR = \"data/cpsc2021\"\n",
                "\n",
                "def setup_data():\n",
                "    if os.path.exists(EXTRACT_DIR) and len(os.listdir(EXTRACT_DIR)) > 0:\n",
                "        print(\"Data directory exists and is not empty. Skipping setup.\")\n",
                "        return\n",
                "\n",
                "    os.makedirs(\"data\", exist_ok=True)\n",
                "\n",
                "    # Load from Google Drive\n",
                "    if os.path.exists(DRIVE_ZIP_PATH):\n",
                "        print(f\"Found data in Drive: {DRIVE_ZIP_PATH}\")\n",
                "        print(\"Unzipping to local runtime...\")\n",
                "        # Unzip to EXTRACT_DIR\n",
                "        os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
                "        subprocess.run([\"unzip\", \"-q\", DRIVE_ZIP_PATH, \"-d\", EXTRACT_DIR], check=True)\n",
                "        print(\"Data loaded from Drive.\")\n",
                "    else:\n",
                "        print(f\"ERROR: Data not found in Drive at {DRIVE_ZIP_PATH}\")\n",
                "        print(\"Please upload 'Training_set_I.zip' to that location in your Google Drive and re-run this cell.\")\n",
                "\n",
                "setup_data()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "import wfdb\n",
                "import scipy.io\n",
                "from tensorflow.keras import layers, models\n",
                "\n",
                "# Set seeds for reproducibility\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data Loader & Processing Logic (CPSC2021)\n",
                "\n",
                "# Constants\n",
                "FS_NATIVE = 200        # CPSC2021 is 200Hz\n",
                "WINDOW_SEC = 2.0       # 2-second window centered on beat\n",
                "WINDOW_SAMPLES = int(WINDOW_SEC * FS_NATIVE) # 400 samples\n",
                "\n",
                "# Top-Level Mapping\n",
                "TOP_LEVEL_CLASSES = ['N', 'V', 'S']\n",
                "TOP_LEVEL_MAP = {c: i for i, c in enumerate(TOP_LEVEL_CLASSES)}\n",
                "\n",
                "SUB_PRIMITIVE_MAP = {} # Label -> Index\n",
                "SUB_TO_TOP_MAP = {}    # SubIndex -> TopIndex\n",
                "\n",
                "def get_top_level_category(label):\n",
                "    # Heuristic mapping for common descriptors based on PhysioNet standard\n",
                "    # Ref: https://archive.physionet.org/physiobank/annotations.shtml\n",
                "    label = str(label).upper()\n",
                "    if 'V' in label or 'PVC' in label: return 'V'\n",
                "    if 'S' in label or 'SPB' in label or 'PAC' in label or 'A' in label: return 'S'\n",
                "    return 'N' # Default to Normal\n",
                "\n",
                "def load_cpsc_data(data_dir=\"data/cpsc2021\", max_files=None):\n",
                "    # CPSC2021 typically has .mat and .hea files (WFDB standard)\n",
                "    # The recursive download might put files in nested folders like Training_set_I\n",
                "    # We should search recursively or assume a structure.\n",
                "    \n",
                "    mat_files = []\n",
                "    for root, dirs, files in os.walk(data_dir):\n",
                "        for file in files:\n",
                "            if file.endswith(\".mat\"):\n",
                "                mat_files.append(os.path.join(root, file))\n",
                "    \n",
                "    # mat_files = sorted([f for f in os.listdir(data_dir) if f.endswith('.mat')])\n",
                "    mat_files = sorted(mat_files)\n",
                "    \n",
                "    if max_files:\n",
                "        mat_files = mat_files[:max_files]\n",
                "\n",
                "    X_all = []\n",
                "    y_sub_all = [] \n",
                "\n",
                "    print(f\"Scanning {len(mat_files)} files in {data_dir}...\")\n",
                "\n",
                "    for idx, mat_path in enumerate(mat_files):\n",
                "        # base_name = os.path.splitext(mat_file)[0]\n",
                "        # mat_path = os.path.join(data_dir, mat_file)\n",
                "        base_path = os.path.splitext(mat_path)[0]\n",
                "        hea_path = base_path\n",
                "        atr_path = base_path\n",
                "\n",
                "        try:\n",
                "            # 1. Load Annotations (WFDB)\n",
                "            try:\n",
                "                ann = wfdb.rdann(atr_path, 'atr')\n",
                "                indices = ann.sample\n",
                "                symbols = ann.symbol\n",
                "                if not symbols:\n",
                "                    continue\n",
                "            except Exception:\n",
                "                continue\n",
                "\n",
                "            # 2. Load Signal\n",
                "            try:\n",
                "                sig, fields = wfdb.rdsamp(hea_path)\n",
                "                fs = fields['fs']\n",
                "                if fs != FS_NATIVE:\n",
                "                    # Naive skip for now if FS mismatch\n",
                "                    pass \n",
                "            except:\n",
                "                 continue\n",
                "\n",
                "            # Use Lead I (Index 0)\n",
                "            signal_1d = sig[:, 0]\n",
                "\n",
                "            for i, samp in enumerate(indices):\n",
                "                sym = symbols[i]\n",
                "                \n",
                "                # Skip non-beat annotations\n",
                "                if sym in ['+', '~', '\"', '!', '[', ']', '|', 'x', '(', ')', 'p', 't', 'u', '`', \"'\"]:\n",
                "                    continue\n",
                "\n",
                "                # Windowing\n",
                "                start = samp - WINDOW_SAMPLES // 2\n",
                "                end = start + WINDOW_SAMPLES\n",
                "\n",
                "                if start < 0 or end > len(signal_1d):\n",
                "                    continue\n",
                "\n",
                "                segment = signal_1d[start:end]\n",
                "                \n",
                "                # Normalize\n",
                "                segment = (segment - np.mean(segment)) / (np.std(segment) + 1e-6)\n",
                "\n",
                "                X_all.append(segment)\n",
                "                y_sub_all.append(sym)\n",
                "\n",
                "        except Exception as e:\n",
                "            pass\n",
                "\n",
                "        if (idx+1) % 50 == 0:\n",
                "            print(f\"Processed {idx+1} files...\")\n",
                "\n",
                "    if not X_all:\n",
                "        return None, None, None\n",
                "\n",
                "    X = np.array(X_all)\n",
                "    X = X[..., np.newaxis] # (N, 400, 1)\n",
                "    \n",
                "    # Dynamic Label Mapping\n",
                "    unique_labels = sorted(list(set(y_sub_all)))\n",
                "    print(f\"Found unique sub-primitives: {unique_labels}\")\n",
                "\n",
                "    global SUB_PRIMITIVE_MAP, SUB_TO_TOP_MAP\n",
                "    SUB_PRIMITIVE_MAP = {l: i for i, l in enumerate(unique_labels)}\n",
                "    \n",
                "    # Build mapping to top-level\n",
                "    for l, idx in SUB_PRIMITIVE_MAP.items():\n",
                "        top_cat = get_top_level_category(l)\n",
                "        SUB_TO_TOP_MAP[idx] = TOP_LEVEL_MAP[top_cat]\n",
                "\n",
                "    y_indices = np.array([SUB_PRIMITIVE_MAP[l] for l in y_sub_all])\n",
                "    \n",
                "    # Convert to One-Hot\n",
                "    num_subs = len(unique_labels)\n",
                "    y_onehot = np.eye(num_subs)[y_indices]\n",
                "\n",
                "    return X, y_onehot, y_indices\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model Architecture (Updated for 200Hz Input)\n",
                "def build_model(input_shape, num_classes):\n",
                "    inputs = layers.Input(shape=input_shape)\n",
                "\n",
                "    x = layers.Conv1D(64, 15, strides=2, padding='same', activation='relu')(inputs)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.MaxPooling1D(3, strides=2, padding='same')(x)\n",
                "\n",
                "    # Residual Blocks\n",
                "    for filters in [64, 128, 256, 512]:\n",
                "        shortcut = x\n",
                "        x = layers.Conv1D(filters, 7, padding='same', activation='relu')(x)\n",
                "        x = layers.BatchNormalization()(x)\n",
                "        x = layers.Conv1D(filters, 7, padding='same', activation='relu')(x)\n",
                "        x = layers.BatchNormalization()(x)\n",
                "        \n",
                "        if shortcut.shape[-1] != filters:\n",
                "            shortcut = layers.Conv1D(filters, 1, padding='same')(shortcut)\n",
                "        \n",
                "        x = layers.Add()([x, shortcut])\n",
                "        x = layers.MaxPooling1D(2)(x)\n",
                "\n",
                "    x = layers.GlobalAveragePooling1D()(x)\n",
                "    x = layers.Dense(128, activation='relu')(x)\n",
                "    x = layers.Dropout(0.5)(x)\n",
                "    \n",
                "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
                "\n",
                "    model = models.Model(inputs=inputs, outputs=outputs, name=\"ECG_CPSC2021_ResNet\")\n",
                "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
                "    return model\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training & Evaluation Loop\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "\n",
                "def train_and_eval():\n",
                "    X, y, y_indices = load_cpsc_data()\n",
                "    \n",
                "    if X is None:\n",
                "        print(\"No valid data loaded. Please check if CPSC2021 contains beat annotations.\")\n",
                "        return\n",
                "    \n",
                "    # Stratify by 1D indices\n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y_indices)\n",
                "    \n",
                "    print(f\"Train Shape: {X_train.shape}, Test Shape: {X_test.shape}\")\n",
                "    print(f\"Classes Map: {SUB_PRIMITIVE_MAP}\")\n",
                "\n",
                "    model = build_model(X_train.shape[1:], y_train.shape[1])\n",
                "    \n",
                "    history = model.fit(\n",
                "        X_train, y_train,\n",
                "        validation_data=(X_test, y_test),\n",
                "        epochs=10,\n",
                "        batch_size=64,\n",
                "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
                "    )\n",
                "    \n",
                "    # Evaluation\n",
                "    print(\"\\n--- Sub-Primitive Evaluation ---\")\n",
                "    y_pred_prob = model.predict(X_test)\n",
                "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
                "    y_true = np.argmax(y_test, axis=1)\n",
                "    \n",
                "    sub_labels = sorted(SUB_PRIMITIVE_MAP.values())\n",
                "    sub_names = [k for k, v in sorted(SUB_PRIMITIVE_MAP.items(), key=lambda item: item[1])]\n",
                "    print(classification_report(y_true, y_pred, labels=sub_labels, target_names=sub_names, zero_division=0))\n",
                "\n",
                "    print(\"\\n--- Top-Level Primitive Evaluation (N, V, S) ---\")\n",
                "    y_true_top = np.array([SUB_TO_TOP_MAP[i] for i in y_true])\n",
                "    y_pred_top = np.array([SUB_TO_TOP_MAP[i] for i in y_pred])\n",
                "    \n",
                "    top_labels = sorted(TOP_LEVEL_MAP.values())\n",
                "    top_names = [k for k, v in sorted(TOP_LEVEL_MAP.items(), key=lambda item: item[1])]\n",
                "    print(classification_report(y_true_top, y_pred_top, labels=top_labels, target_names=top_names, zero_division=0))\n",
                "    \n",
                "    model.save(\"models/cpsc2021_beat_model.keras\")\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    os.makedirs(\"models\", exist_ok=True)\n",
                "    if os.path.exists(\"data/cpsc2021\"):\n",
                "        train_and_eval()\n",
                "    else:\n",
                "        print(\"Data directory not found. Please ensure download completes.\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert to TFLite\n",
                "def convert_to_tflite(model_path=\"models/cpsc2021_beat_model.keras\", tflite_path=\"models/cpsc2021_beat_model.tflite\"):\n",
                "    print(f\"Converting {model_path} to TFLite...\")\n",
                "    try:\n",
                "        model = tf.keras.models.load_model(model_path)\n",
                "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
                "        tflite_model = converter.convert()\n",
                "        \n",
                "        with open(tflite_path, \"wb\") as f:\n",
                "            f.write(tflite_model)\n",
                "        print(f\"TFLite model saved to {tflite_path}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error converting to TFLite: {e}\")\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    if os.path.exists(\"models/cpsc2021_beat_model.keras\"):\n",
                "        convert_to_tflite()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download Models (Colab specific)\n",
                "try:\n",
                "    from google.colab import files\n",
                "    print(\"Zipping models directory...\")\n",
                "    !zip -r models.zip models\n",
                "    print(\"Downloading models.zip...\")\n",
                "    files.download(\"models.zip\")\n",
                "except ImportError:\n",
                "    print(\"Not running in Google Colab, skipping download.\")\n"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}